{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_outh_file(outh_filename):\n",
    "    keys = []\n",
    "    with open(outh_filename) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            keys.append(line.strip('\\n'))\n",
    "            line = fp.readline()\n",
    "\n",
    "    consumer_key = keys[0]\n",
    "    consumer_secret_key = keys[1]\n",
    "    access_token = keys[2]\n",
    "    access_secret_token = keys[3]\n",
    "    \n",
    "    return consumer_key,consumer_secret_key,access_token,access_secret_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autorize_twitter_api():\n",
    "\n",
    "    consumer_key,consumer_secret_key,access_token,access_secret_token = read_outh_file(\"outh.txt\")\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "    auth.set_access_token(access_token, access_secret_token)\n",
    "    \n",
    "    return auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_graph(graph_df):\n",
    "    \n",
    "    dff = graph_df.sample(frac=1).tail(1000)\n",
    "\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    for index, data in dff.iterrows():\n",
    "        user = data['user_id']\n",
    "        graph.add_node(user)\n",
    "        graph.node[user][\"name\"] = data['screen_name']\n",
    "        if data['retweeted_id'] is not None:\n",
    "            retweet = data['retweeted_id']\n",
    "            graph.add_edge(user, retweet)\n",
    "            graph.node[retweet][\"name\"] = data['retweeted_screen_name']\n",
    "\n",
    "        elif data['in_reply_to_user_id'] is not None:\n",
    "            reply_user = data['in_reply_to_user_id']\n",
    "            graph.add_edge(user, reply_user)\n",
    "            graph.node[reply_user][\"name\"] = data['in_reply_to_screen_name']\n",
    "\n",
    "        elif data['in_reply_to_status_id'] is not None:\n",
    "            reply_status = data['in_reply_to_status_id']\n",
    "            graph.add_edge(user, reply_status)\n",
    "            graph.node[reply_status][\"name\"] = data['in_reply_to_screen_name']\n",
    "\n",
    "        elif data['user_mentions_id'] is not None:\n",
    "            user_mentions = data['user_mentions_id']\n",
    "            graph.add_edge(user, user_mentions)\n",
    "            graph.node[user_mentions][\"name\"] = data['user_mentions_screen_name']\n",
    "\n",
    "    pos = nx.spring_layout(graph, k=0.05)\n",
    "    for n, p in pos.items():\n",
    "        graph.node[n]['pos'] = p\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_n_users_page_rank(graph, top_n_users = None):\n",
    "    \n",
    "    if top_n_users is None:\n",
    "        top_n_users = 10\n",
    "    \n",
    "    \n",
    "    pr = nx.pagerank(graph)\n",
    "    sorted_nodes = sorted([(node, pagerank) for node, pagerank in pr.items()], key=lambda x:pr[x[0]])\n",
    "    \n",
    "    ids_of_sorted_nodes = [x[0] for x in sorted_nodes][:top_n_users]\n",
    "\n",
    "    list_users = []\n",
    "    for id_sorted in ids_of_sorted_nodes:\n",
    "        name = graph.node[id_sorted]['name']\n",
    "        list_users.append(name)\n",
    "        \n",
    "    df = pd.DataFrame(list_users, columns=[\"Top User Ranking\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_searched_terms(data):\n",
    "    cleaned_text = [text.split('_', 1)[0] for text in data]\n",
    "    unique_terms = list(set(cleaned_text))\n",
    "    unique_terms.sort()\n",
    "    return unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_colors(sentiment):\n",
    "    if sentiment == \"Positive\":\n",
    "        return \"#00ff00\"\n",
    "    elif sentiment == \"Negative\":\n",
    "        return \"#ff0000\"\n",
    "    else:\n",
    "        return '#ffff00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_word_frequency(data):\n",
    "    no_url_text = [\" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", text).split()) for text in data]\n",
    "    cleaned_text = [text.lower().split(' ') for text in no_url_text]\n",
    "    flattened_list = [item for sublist in cleaned_text for item in sublist]\n",
    "    \n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    stop_words.append('rt')\n",
    "    \n",
    "    remove_stopwords = [word for word in flattened_list if not word in stop_words]\n",
    "             \n",
    "    \n",
    "    count_unique_words = collections.Counter(remove_stopwords)\n",
    "    \n",
    "    return count_unique_words.most_common(15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_frequent_source(data):\n",
    "    clean = re.compile('<.*?>')\n",
    "    no_url_text = [re.sub(clean, '', text) for text in data]\n",
    "    count_unique_source = collections.Counter(no_url_text)\n",
    "    return count_unique_source.most_common(15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_frequent_hashtags(data):\n",
    "\n",
    "    remove_text = [\"\".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", text)) for text in data]\n",
    "    remove_null = [item for item in remove_text if len(item) != 0]\n",
    "    count_hashtags = collections.Counter(remove_null)\n",
    "    return count_hashtags.most_common(15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_frequent_users(data):\n",
    "    count_unique_users = collections.Counter(data)\n",
    "    return count_unique_users.most_common(15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
